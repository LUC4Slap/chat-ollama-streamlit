import streamlit as st
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from database.save_context import salvar_contexto, recuperar_ultimo_contexto, recuperar_mensagens_do_contexto
import os

# Configura√ß√£o do modelo com streaming
# model = OllamaLLM(model="cogito", streaming=True)
model = OllamaLLM(model="cogito", base_url=os.getenv("OLLAMA_URL"), streaming=True)

# Template do prompt
template = """
Voc√™ √© um assistente de programa√ß√£o altamente qualificado, capaz de explicar conceitos, corrigir c√≥digos, sugerir boas pr√°ticas e resolver d√∫vidas sobre qualquer linguagem ou tecnologia de desenvolvimento de software.

Voc√™ sempre responde de forma did√°tica, t√©cnica e, quando necess√°rio, apresenta exemplos de c√≥digo claros e funcionais.

Aqui est√° o hist√≥rico da nossa conversa at√© agora: {context}

Pergunta do usu√°rio: {question}

Sua resposta:
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(page_title="Yoda AI ChatBot", page_icon="üõ∏")
st.title("üõ∏ Yoda AI ChatBot")
contexto = recuperar_ultimo_contexto("padawan")
# Inicializa o estado da aplica√ß√£o
if "messages" not in st.session_state:
    st.session_state.messages = recuperar_mensagens_do_contexto(contexto)
if "context" not in st.session_state:
    st.session_state.context = contexto or ""

# Exibe o hist√≥rico de mensagens
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Campo de entrada do usu√°rio
if prompt := st.chat_input("Digite sua pergunta..."):
    # Adiciona pergunta ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        streamed_response = ""

        for chunk in chain.stream({"context": st.session_state.context, "question": prompt}):
            streamed_response += chunk
            response_placeholder.markdown(streamed_response + "‚ñå")

        response_placeholder.markdown(streamed_response)
        st.session_state.messages.append({"role": "assistant", "content": streamed_response})

        # Atualiza contexto
        st.session_state.context += f"\nUser: {prompt}\nAI: {streamed_response}"
        salvar_contexto("padawan", st.session_state.context)
